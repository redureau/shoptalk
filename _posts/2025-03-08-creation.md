---
title: "Why some machine learning models overfit more than others?"
date: 2025-03-08
---

# Introduction

Overfitting is not only about the number of parameters or even the VC dimension. Transformers are notorious for indefinitely improving, which suggests a more nuanced notion of expressiveness.

# Framework

Let's recall a cannonical machine learning framework:
$X$ and $Y$ are random variables, and we are trying to predict $Y$ given $X$, that is to say we are interested in the conditional probability distribution $Y|X$.
We define a model $f$ as an application from $X$ support (it is actually not necessarily toward $Y$ support, $f(X)$ could be a parameter to describe $Y|X$). The model is often parametric, and we define the hypothesis class (set of models): $\mathcal{H}=\\{f_\theta, \theta \in \Theta \\}$.
We introduce a scalar loss function to assess the quality of our estimation: $\mathcal{L}(f(x),y)$
Finally, a model is evaluated by its risk: $R[f_\theta]=E[\mathcal{L}(f_\theta(x),y)]$.
We note $\tilde{f}$ the element of $\mathcal{H}$ that minimizes the risk.
$X$ and $Y$ are observed through samples, like: $S=\\{(x_i,y_i)_i\\}$, then $f\tilde{f}$ and $R$ are only estimated.

> I briefly introduced those notions as they are common, some detail oriented questions that came to my mind while writing this:
> - Is the right tool to describe $Y|X$ markov kernels ?
> - $f_\theta$ naturally introduces a pushforward measure ?  
> I am not familiar with these notions, but I believe they may contain nuances that I might miss.

# Introduction to the bias/variance tradeoff via an example

I hate made-up examples, except if they effectively emphasize a point. Since I don't see such one for now, I will introduce the bias/variance tradeoff with a multinomial model for MNIST.

<!-- Local multinomial ? -->

<!-- VC dim of SVM is # number of dimension -->

# A theoretical framework: VC dimension

# Why early stopping works ? An interpretation

# Regularity also matters

<!-- Momentum, reconciling, Model complexity control for regression using VC generalization bounds -->
